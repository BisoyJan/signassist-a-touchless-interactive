================================================================================
  SignAssist — Full Setup Guide
  Touchless FSL Sign Language Recognition & Text-to-Speech System
================================================================================

PREREQUISITES
--------------------------------------------------------------------------------
  - Node.js >= 18 (recommended: 20 LTS or later)
  - npm (comes with Node.js)
  - Python >= 3.10 (for model training only)
  - Git
  - A webcam (for live gesture recognition and data collection)
  - Google Chrome (recommended for kiosk deployment)

IMPORTANT (Camera / Browser Security)
--------------------------------------------------------------------------------
  Webcam access works only on secure contexts:
    - https:// OR
    - http://localhost

  If you open the site from a LAN IP over plain http (e.g., http://192.168.x.x:3000)
  or from file://, Chrome will block the camera.


================================================================================
  STEP 1: Clone the Repository
================================================================================

  git clone https://github.com/<your-username>/SignAssist-A-Touchless-Interactive.git
  cd SignAssist-A-Touchless-Interactive


================================================================================
  STEP 2: Install Node.js Dependencies
================================================================================

  npm install

  This installs Next.js, React, TailwindCSS, MediaPipe, TensorFlow.js, etc.


================================================================================
  STEP 3: Run the Development Server
================================================================================

  npm run dev

  - Open http://localhost:3000 in Chrome
  - Allow camera permissions when prompted
  - The main kiosk page will load at /
  - The data collection page is at /collect

  QUICK RUN (skip training)
--------------------------------------------------------------------------------
  This repo already includes a TF.js model in:
    public/models/lstm/

  So you can run the kiosk immediately without Python, as long as the browser can
  access the camera.


================================================================================
  STEP 4: Collect Training Data
================================================================================

  1. Open http://localhost:3000/collect in Chrome
  2. Select a sign label from the vocabulary list
  3. Perform the sign in front of the webcam
  4. The app records 30 frames of hand landmarks per sample
  5. Collect at least 50-100 samples per sign for good accuracy
  6. Click "Download Samples" to save the JSON file
  7. Place the downloaded JSON file(s) in the training/data/ folder


================================================================================
  STEP 5: Set Up Python Environment (for Training)
================================================================================

  a) Navigate to the training folder:

     cd training

  b) Create a virtual environment:

     python -m venv .venv

  c) Activate the virtual environment:

     Windows (Command Prompt):
       .venv\Scripts\activate

     Windows (PowerShell):
       .venv\Scripts\Activate.ps1

     macOS / Linux:
       source .venv/bin/activate

  d) Install Python dependencies:

     pip install -r requirements.txt

     This installs:
       - tensorflow >= 2.15.0
       - tensorflowjs >= 4.17.0
       - numpy >= 1.24.0
       - scikit-learn >= 1.3.0
       - opencv-python >= 4.8.0   (for video extraction)
       - mediapipe >= 0.10.9      (for video extraction)

  e) Go back to project root:

     cd ..

  (Optional) If you want to run training from the project root without manually
  "cd training" first, activate the venv like this:

    Windows (Command Prompt):
      training\.venv\Scripts\activate

    Windows (PowerShell):
      .\training\.venv\Scripts\Activate.ps1

    Git Bash (Windows):
      source training/.venv/Scripts/activate


================================================================================
  STEP 6a: (Option A) Train from Collected Samples
================================================================================

  Make sure:
    - The Python virtual environment is activated
    - You have JSON sample files in training/data/
    - You installed dependencies: pip install -r training/requirements.txt

  Run the training script:

     python training/train_model.py


================================================================================
  STEP 6b: (Option B) Train from Video Files
================================================================================

  Instead of (or in addition to) the /collect page, you can train from
  pre-recorded video files of sign language gestures.

  1. Create subfolders in training/videos/ — one per gesture label:

     training/videos/
       hello/
         clip1.mp4
         clip2.mp4
       thank_you/
         clip1.mp4
       letter_a/
         clip1.mp4
         clip2.mp4

     Folder names must match gesture labels in src/config/index.ts.
     Supported formats: .mp4, .avi, .mov, .mkv, .webm

  2. Recording tips:
     - Record 2–5 second clips per gesture
     - Aim for 10+ clips per gesture for good accuracy
     - Use the same camera angle as the kiosk (front-facing, chest-up)
     - Vary lighting (bright + dim) to match real conditions
     - Include both hands if the sign uses them
     - Add an "unknown/" folder with random non-gesture movements

  3. Extract hand landmarks from the videos:

     python training/extract_from_video.py --videos_dir ./videos

     This uses MediaPipe to detect hand landmarks in every video frame,
     builds sliding-window sequences (30 frames, 50% overlap), and outputs
     a JSON file to training/data/video_samples.json — the same format as
     the /collect page, so train_model.py picks it up automatically.

     Options:
       --augment         Enable data augmentation (3x more samples via
                         jitter, scale, shift)
       --overlap 0.75    Increase sliding window overlap (default: 0.5)
       --signer Maria    Tag samples with a signer name
       --output ./data/custom_name.json   Custom output path

     Example with augmentation:

       python training/extract_from_video.py --videos_dir ./videos --augment

  4. Train the model (same as Step 6a):

     python training/train_model.py

     The script loads ALL JSON files in training/data/ — both /collect
     samples and video-extracted samples are combined automatically.

  What the script does:
    1. Loads all JSON sample files from training/data/
    2. Builds a Bidirectional LSTM model
    3. Trains with early stopping and learning rate reduction
    4. Evaluates on a held-out test set
    5. Saves the Keras model to training/model.keras
    6. Converts to TensorFlow.js format in public/models/lstm/
    7. Copies labels.json to public/models/lstm/

  After training, the model files will be at:
    public/models/lstm/model.json       (TF.js model)
    public/models/lstm/labels.json      (class labels)
    public/models/lstm/group1-shard*.bin (model weights)


================================================================================
  STEP 7: Build for Production (Static Export)
================================================================================

  npm run build

  This generates a fully static export in the out/ folder.
  No server required — just serve the out/ folder with any static file server.


================================================================================
  STEP 8: Deploy as Kiosk
================================================================================

  Option A — Chrome Kiosk Mode (recommended):

    chrome --kiosk --disable-pinch --overscroll-history-navigation=0 http://localhost:3000

  Option B — Serve the static build:

    npx serve out
    chrome --kiosk http://localhost:3000

  Note:
    - If you serve the static build from another device/host, use https:// to
      allow camera access, or keep it on the kiosk machine at http://localhost.

  Option C — Deploy to any static hosting (Vercel, Netlify, GitHub Pages, etc.):

    Upload the contents of the out/ folder.


================================================================================
  PROJECT STRUCTURE OVERVIEW
================================================================================

  src/app/page.tsx             — Main kiosk page
  src/app/collect/page.tsx     — Data collection tool
  src/hooks/useHandTracker.ts  — MediaPipe hand detection & webcam
  src/hooks/useGestureClassifier.ts — TF.js LSTM inference
  src/hooks/useSpeechOutput.ts — Web Speech API TTS
  src/components/              — UI components
  src/config/index.ts          — FSL vocabulary & system config
  training/train_model.py      — Python training pipeline
  training/extract_from_video.py — Extract landmarks from video files
  training/data/               — Training sample JSON files
  training/videos/             — Video files for training (organized by label)
  public/models/lstm/          — TF.js model files (output of training)


================================================================================
  USEFUL COMMANDS QUICK REFERENCE
================================================================================

  npm install                        — Install Node.js dependencies
  npm run dev                        — Start development server
  npm run build                      — Build static export to out/
  npm run lint                       — Run ESLint

  cd training
  python -m venv .venv               — Create Python virtual environment
  .venv\Scripts\activate             — Activate venv (Windows CMD)
  pip install -r requirements.txt    — Install Python training dependencies
  python train_model.py              — Train the LSTM model
  python extract_from_video.py       — Extract landmarks from video files
  python extract_from_video.py --augment — Extract with data augmentation
  deactivate                         — Deactivate the virtual environment


================================================================================
  TROUBLESHOOTING
================================================================================

  Camera not working?
    - Make sure Chrome has camera permissions
    - Check that no other app is using the webcam

  Model not loading in the browser?
    - Ensure public/models/lstm/model.json exists
    - Re-run: python training/train_model.py

  Low accuracy?
    - Collect more samples (100+ per sign recommended)
    - Ensure consistent hand positioning during data collection
    - Check that lighting conditions are adequate

  Python/TensorFlow issues?
    - Use Python 3.10-3.12 (TensorFlow compatibility)
    - Make sure the virtual environment is activated before installing

================================================================================
